{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from brown import Brown\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size : 101\n",
      "Corpus Size : 56610\n"
     ]
    }
   ],
   "source": [
    "word2idx, sentences = Brown().get_limited_vocab(vocab_size=100)\n",
    "print(\"Vocabulary Size : {}\".format(len(word2idx)))\n",
    "print(\"Corpus Size : {}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = word2idx[\"START\"]\n",
    "end_idx = word2idx[\"END\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_probs(start_idx, end_idx, V, sentences, smoothning=1.0):\n",
    "    \"\"\"\n",
    "    ---------------------------------------------------------------\n",
    "    Description :\n",
    "    Calculates the bigram probability of sentences\n",
    "    \n",
    "    Input:\n",
    "    start_idx : index of \"START\" tag\n",
    "    end_idx : index of \"END\" tag\n",
    "    V : vocab size\n",
    "    sentences : list of sentence\n",
    "    smoothning : amt. of smoothning to be applied to the bigram probabilities\n",
    "    \n",
    "    Return :\n",
    "    bigram_probs : a numpy ndarray of size (V x V) containing the bigram probabilites of sentences\n",
    "    -----------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    bigram_probs = np.ones((V, V)) * smoothning\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            \n",
    "            if i == 0:\n",
    "                bigram_probs[start_idx, words[i]] += 1\n",
    "            \n",
    "            elif i == len(words) - 1:\n",
    "                bigram_probs[words[i], end_idx] += 1\n",
    "            \n",
    "            else:\n",
    "                bigram_probs[words[i - 1], words[i]] += 1\n",
    "                \n",
    "    \n",
    "    bigram_probs /= bigram_probs.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return bigram_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(word2idx)\n",
    "W_bigram = get_bigram_probs(start_idx, end_idx, V, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(V, V) / np.sqrt(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    ------------------------------------------------\n",
    "    Description :\n",
    "    Function to calculate the softmax function \n",
    "    \n",
    "    Input :\n",
    "    z : a numpy ndarray\n",
    "    \n",
    "    Return :\n",
    "    soft : the softmax value of the ndarray\n",
    "    -------------------------------------------------\n",
    "    \"\"\"\n",
    "    z = z - z.max()\n",
    "    soft = np.exp(z) / np.sum(np.exp(z), axis = 1, keepdims = True) \n",
    "    return soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W):\n",
    "    \"\"\"\n",
    "    -------------------------------------------------\n",
    "    Description:\n",
    "    Function to calculate the dot product of two vectors\n",
    "    \n",
    "    Input:\n",
    "    X : a numpy ndarray of size (N x V)\n",
    "    W : a numpy ndarray of size (V x V)\n",
    "    \n",
    "    Return :\n",
    "    z : a numpy a ndarray of size (N x V)\n",
    "    --------------------------------------------------\n",
    "    \"\"\"\n",
    "    z = np.dot(X,W)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X, P, Y):\n",
    "    \"\"\"\n",
    "    ---------------------------------------------\n",
    "    Description :\n",
    "    Function to calculate the backpropagation of the neural network\n",
    "    \n",
    "    Input : \n",
    "    X : a numpy ndarray of size (N x V)\n",
    "    P : a numpy ndarray of size (N x V)\n",
    "    Y : a numpy ndarray of size (N x V)\n",
    "    \n",
    "    Return :\n",
    "    dW : a numpy ndarray of size (V x V)\n",
    "    ---------------------------------------------\n",
    "    \"\"\"\n",
    "    dW = X.T.dot(P - Y)\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_bigram = np.log(W_bigram) ## This should be the ideal weight that the model is trying to achieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning:\n",
      "\n",
      "This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d9f248d09a40879d74be15ac8f4c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=56610.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch : 0, Loss : 2.3600890722541243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919f9e4b0d1042a9ae3eb7eb6c2733b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=56610.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch : 1, Loss : 2.2251576157852524\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d88fdbc90f944128eea0ae8c6d3256a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=56610.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch : 2, Loss : 2.206161660817967\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "learning_rate = 0.01\n",
    "losses = []\n",
    "for i in range(epochs):\n",
    "    random.shuffle(sentences)\n",
    "    epoch_loss = 0\n",
    "    j = 0\n",
    "    with tqdm(total=len(sentences)) as pbar:\n",
    "        for sentence in sentences:\n",
    "            complete_sentence = [start_idx] + sentence + [end_idx]\n",
    "            n = len(complete_sentence)\n",
    "            input_sentence = np.zeros((n-1, V))\n",
    "            output_sentence = np.zeros((n-1, V))\n",
    "            # One hot encoding the input and output\n",
    "            input_sentence[np.arange(n - 1), complete_sentence[:n-1]] =  1\n",
    "            output_sentence[np.arange(n - 1), complete_sentence[1:]] =  1\n",
    "\n",
    "            z = forward(input_sentence, W)\n",
    "            z = softmax(z)\n",
    "            dW = backprop(input_sentence, z, output_sentence)\n",
    "            W = W - learning_rate * dW\n",
    "            loss = -np.sum(output_sentence * np.log(z)) / (n - 1)\n",
    "            losses.append(loss)        \n",
    "            j += 1\n",
    "            epoch_loss += loss\n",
    "            pbar.update(1)\n",
    "    epoch_loss /= j\n",
    "    \n",
    "    print(\"Epoch : {}, Loss : {}\".format(i, epoch_loss))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
