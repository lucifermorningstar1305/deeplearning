{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "brown_sentences = brown.sents()\n",
    "print(brown_sentences[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_sentences = list(map(lambda x: ' '.join(x).lower(), brown_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_limited(vocab_size = 2000):\n",
    "    \"\"\" \n",
    "    -----------------------------\n",
    "    Description :\n",
    "    Function to return a limited vocabulary\n",
    "    \n",
    "    Input : \n",
    "    vocab_size : The vocabulary size required\n",
    "    \n",
    "    Return :\n",
    "    sentence_small : The sentences consisting of the limited vocabulary (list structure)\n",
    "    word2idx_small : Limited vocabularies (dict structure)\n",
    "    ------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    word2idx = {\"START\" : 0, \"END\" : 1}\n",
    "    idx2word = [\"START\", \"END\"]\n",
    "    word2idx_count = {0:float('inf'), 1:float('inf')}\n",
    "    \n",
    "    indexed_sentences = []\n",
    "    sentences_small = []\n",
    "    word2idx_small = {}\n",
    "    n = 0\n",
    "    \n",
    "    for sentences in brown_sentences:\n",
    "        # sentences : a string value  eg. \"The quick brown fox jumps over the lazy dog\"\n",
    "        \n",
    "        words = sentences.split()\n",
    "        \n",
    "        # words : list structure [\"The\", \"quick\", \"brown\" ....]\n",
    "        indexed_sentence = []\n",
    "        \n",
    "        for token in words:\n",
    "            \n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = n\n",
    "                n += 1\n",
    "                idx2word.append(token)\n",
    "                word2idx_count[word2idx[token]] = 1\n",
    "                \n",
    "            word2idx_count[word2idx[token]] += 1\n",
    "            indexed_sentence.append(word2idx[token])\n",
    "            \n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "            \n",
    "    \n",
    "    new_sorted_vocab = sorted([(k,v) for k,v in word2idx_count.items()], key=operator.itemgetter(1), reverse = True)\n",
    "    # new_sorted_vocab : list structure [(1, 200), (2, 500) ....] where the first index of every tuple is the word-index and the second index is it's count\n",
    "    \n",
    "    new_idx = 0\n",
    "    new_idx2idx_map = {}\n",
    "    \n",
    "    for k,v in new_sorted_vocab[:vocab_size]:\n",
    "        if k not in word2idx_small:\n",
    "            word = idx2word[k]\n",
    "            word2idx_small[word] = new_idx\n",
    "            \n",
    "            #Creating a new map of index to index as we are selecting a certain amount of word\n",
    "\n",
    "            new_idx2idx_map[word2idx[word]] = new_idx\n",
    "            new_idx += 1\n",
    "        \n",
    "    word2idx_small[\"UNKNOWN\"] = new_idx\n",
    "    unknown = new_idx\n",
    "    \n",
    "    # indexed_sentence : list of list structure \n",
    "    for sentence in indexed_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [new_idx2idx_map[idx] if idx in new_idx2idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "            \n",
    "    \n",
    "    return sentences_small, word2idx_small\n",
    "        \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_prob(sentences, V, start_idx, end_idx, smoothning=1):\n",
    "    \"\"\"\n",
    "    --------------------------------------------------\n",
    "    Description :\n",
    "    Function to form the bigram model of a sentence\n",
    "    \n",
    "    Input :\n",
    "    sentences : list of list structure \n",
    "    V : Vocab size\n",
    "    start_idx : Index value of \"START\" token\n",
    "    end_idx : Index value of \"END\" token\n",
    "    smoothning : smoothning value to be added\n",
    "    \n",
    "    Return :\n",
    "    mat : n-dimensional matrix (The Bigram Model)\n",
    "    -------------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    mat = np.ones((V, V)) * smoothning # Add-one smoothining hence intializing the matrix with ones\n",
    "    for sentence in sentences:\n",
    "        # sentence : list of integers\n",
    "        words = sentence\n",
    "        for i in range(len(words)):\n",
    "            if i == 0:\n",
    "                mat[start_idx, words[i]] += 1\n",
    "            \n",
    "            elif i == len(words) - 1:\n",
    "                mat[words[i], end_idx] += 1\n",
    "            \n",
    "            else:\n",
    "                mat[words[i - 1], words[i]] += 1\n",
    "                \n",
    "    mat /= mat.sum(axis = 1, keepdims = True)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 10001\n"
     ]
    }
   ],
   "source": [
    "sentences, word2idx = get_vocab_limited(vocab_size=10000)\n",
    "print(\"Vocab size : {}\".format(len(word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Bigram probabilities : (10001, 10001)\n"
     ]
    }
   ],
   "source": [
    "start_idx = word2idx[\"START\"]\n",
    "end_idx = word2idx[\"END\"]\n",
    "bigram_mat = bigram_prob(sentences, len(word2idx), start_idx, end_idx)\n",
    "print(\"Shape of Bigram probabilities : {}\".format(bigram_mat.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(sentence):\n",
    "    \"\"\" \n",
    "    --------------------------------------\n",
    "    Description:\n",
    "    Function to determine the log-probability score of a sentence \n",
    "    \n",
    "    Input:\n",
    "    sentence : list of integers containing indices of each word in a sentence\n",
    "    \n",
    "    Return:\n",
    "    prob_score : log probability score of a sentence\n",
    "    ---------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    # score calculation = (log p(w_1) + summation(p(w_t | w_t-1))) / T\n",
    "    for i in range(len(sentence)):\n",
    "        if i == 0:\n",
    "            score += np.log(bigram_mat[start_idx, sentence[i]])\n",
    "        else:\n",
    "            score += np.log(bigram_mat[sentence[i - 1], sentence[i]])\n",
    "    \n",
    "    # For the end \n",
    "    score += np.log(bigram_mat[sentence[-1], end_idx])\n",
    "    prob_score = score / (len(sentence) + 1) \n",
    "    return prob_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(sentence):\n",
    "    return ' '.join([idx2word[i] for i in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Sentence : `` i don't UNKNOWN '' , i told UNKNOWN , `` except that i UNKNOWN be here '' . | Score : -4.5254852\n",
      "Fake Sentence : two-inch armour lucifer vientiane hegel's wackers' ratification friday supported perpetually avaliable surprisingly cloakrooms omissions commodity impute 3-inch cut based | Score : -9.3078275\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a text :  i don't know\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered a words which is not in the vocabulary\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to continue Y/N? :  U\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Sentence : u. s. UNKNOWN to UNKNOWN a neutral laos UNKNOWN have led premier UNKNOWN to believe that other areas UNKNOWN be `` neutralized '' on UNKNOWN terms . | Score : -6.0889485\n",
      "Fake Sentence : anti-communists hershey's assigned handed hobbled conspire naked accredited stagecoach lush sprinkle frick gassing holidays york scots inverted seat viewed underwriters coalition tax wilsonian pilot delegating revelation distinctions | Score : -9.2811271\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a text :  a neutral laos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Sentence : a neutral laos | Score : -7.4247563\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to continue Y/N? :  N\n"
     ]
    }
   ],
   "source": [
    "# For fake sentence sampling :\n",
    "sample_p = np.ones(len(word2idx))\n",
    "\n",
    "# When we sample fake sentence we need to make sure not to sample \"START\" or \"END\" token\n",
    "\n",
    "sample_p[start_idx] = 0\n",
    "sample_p[end_idx] = 0\n",
    "sample_p /= sample_p.sum()\n",
    "while True:\n",
    "    real_idx = np.random.choice(len(sentences))\n",
    "    \n",
    "    #sentence : list of list structure (containing indices of words in a sentence)\n",
    "    real = sentences[real_idx]\n",
    "    # Test our bigram model for fake sentence \n",
    "    fake = np.random.choice(len(word2idx), size=len(real), p=sample_p)\n",
    "    \n",
    "    print(\"Real Sentence : {} | Score : {:.7f}\".format(get_words(real), get_score(real)))\n",
    "    print(\"Fake Sentence : {} | Score : {:.7f}\".format(get_words(fake), get_score(fake)))\n",
    "    \n",
    "    \n",
    "    custom_sentence = input(\"Enter a text : \")\n",
    "    custom_sentence = custom_sentence.strip().lower().split()\n",
    "    bad_sentence = False\n",
    "    for words in custom_sentence:\n",
    "        if words not in word2idx:\n",
    "            print(\"You entered have entered words which are not in the vocabulary\")\n",
    "            bad_sentence = True\n",
    "            break\n",
    "    if not bad_sentence:\n",
    "        custom_sentence_list = [word2idx[words] for words in custom_sentence]\n",
    "        print(\"Custom Sentence : {} | Score : {:.7f}\".format(get_words(custom_sentence_list),get_score(custom_sentence_list)))\n",
    "    \n",
    "    cont = input(\"Do you want to continue Y/N? : \")\n",
    "    if cont in ('N', 'n'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
